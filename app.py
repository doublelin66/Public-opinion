import streamlit as st
import google.generativeai as genai
import feedparser
import requests
import json
import pandas as pd
from datetime import datetime

# ================= 1. åŸºç¤è¨­å®š =================
st.set_page_config(
    page_title="å°ç£ç†±é–€è¨è«–",
    page_icon="âš¡",
    layout="wide"
)

# ================= 2. å®‰å…¨è®€å–é‡‘é‘° =================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âš ï¸ æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹ç¢ºèª Streamlit Secrets è¨­å®šã€‚")
    st.stop()

genai.configure(api_key=API_KEY)

# ================= 3. æ ¸å¿ƒåŠŸèƒ½ï¼šå¤šé‡ä¾†æºçˆ¬èŸ² + AI åˆ†æ =================
@st.cache_data(ttl=3600)  # å¿«å– 1 å°æ™‚
def run_analysis():
    # --- A. æ¨¡å‹è¨­å®š (ä½¿ç”¨æ‚¨å¸³è™Ÿæ”¯æ´çš„ 2.5 Flash) ---
    model_name = 'gemini-2.5-flash'
    try:
        model = genai.GenerativeModel(model_name)
    except:
        model = genai.GenerativeModel('gemini-pro')

    # --- B. å®šç¾©å¤šå€‹æ–°èä¾†æº (æ“´å……è³‡è¨Šé‡) ---
    # é€™è£¡åŒ…å«äº† Google æ–°èçš„ä¸‰å¤§åˆ†é¡ï¼šè²¡ç¶“ã€ç§‘æŠ€ã€ç¶œåˆç„¦é»
    rss_sources = {
        "ğŸ’° è²¡ç¶“": "https://news.google.com/rss/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0FBUWlHZ0pKVERNU0FBUW?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "ğŸ¤– ç§‘æŠ€": "https://news.google.com/rss/topics/CAAqKggKIiRDQkFTRlFvSUwyMHZNRGRqTVhZU0FBUWlHZ0pKVERNU0FBUW?hl=zh-TW&gl=TW&ceid=TW%3Azh-Hant",
        "ğŸ”¥ ç„¦é»": "https://news.google.com/rss?hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    }
    
    all_raw_data = []
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    # è¿´åœˆæŠ“å–æ¯ä¸€å€‹ä¾†æº
    for category_name, url in rss_sources.items():
        try:
            response = requests.get(url, headers=headers, timeout=10)
            feed = feedparser.parse(response.content)
            
            # æ¯å€‹åˆ†é¡æŠ“å‰ 10 å‰‡ï¼Œç¸½å…±ç´„ 30 å‰‡
            for entry in feed.entries[:10]:
                all_raw_data.append({
                    "source_channel": category_name, # æ¨™è¨˜æ˜¯å“ªä¸€é¡çš„æ–°è
                    "title": entry.title, 
                    "pubDate": entry.published
                })
        except Exception as e:
            print(f"Error fetching {category_name}: {e}")
            continue

    if not all_raw_data:
        return []

    # --- C. AI åˆ†æ (å‡ç´šç‰ˆ Prompt) ---
    # è®“ AI è™•ç†å¤§é‡è³‡è¨Šï¼šå»é‡ã€åˆ†é¡ã€æ’åº
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è‚¡å¸‚è¼¿æƒ…åˆ†æå¸«ã€‚è«‹é–±è®€ä»¥ä¸‹ä¾†è‡ªä¸åŒé »é“(è²¡ç¶“/ç§‘æŠ€/ç„¦é»)çš„ 30 å‰‡å°ç£æ–°èæ¨™é¡Œã€‚
    è«‹é€²è¡Œæ·±åº¦åˆ†æï¼Œä¸¦ç”¢å‡º JSON æ ¼å¼çš„å ±å‘Šã€‚

    åŸå§‹æ–°èè³‡æ–™ï¼š
    {json.dumps(all_raw_data, ensure_ascii=False)}
    
    ä»»å‹™
