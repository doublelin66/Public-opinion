import streamlit as st
import google.generativeai as genai
import feedparser
import requests
import json
import pandas as pd
from datetime import datetime
import urllib.parse # ç”¨ä¾†è™•ç†ä¸­æ–‡ç¶²å€

# ================= 1. åŸºç¤è¨­å®š =================
st.set_page_config(
    page_title="ğŸ‡¹ğŸ‡¼å°ç£ç†±é–€è¨è«–",
    page_icon="ğŸ”¥",
    layout="wide"
)

# ================= 2. å®‰å…¨è®€å–é‡‘é‘° =================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âš ï¸ æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹ç¢ºèª Streamlit Secrets è¨­å®šã€‚")
    st.stop()

genai.configure(api_key=API_KEY)

# ================= 3. å®šç¾©æ–°èä¾†æº (é—œéµä¿®æ­£ï¼šä¸­æ–‡ç·¨ç¢¼) =================
def get_rss_url(category):
    base_search = "https://news.google.com/rss/search"
    suffix = "hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    # é€™è£¡æœƒç”¨ urllib.parse.quote æŠŠä¸­æ–‡è½‰æˆ URL ç·¨ç¢¼
    # ä¾‹å¦‚ "å°ç£æ”¿æ²»" -> "%E5%8F%B0%E7%81%A3%E6%94%BF%E6%B2%BB"
    def make_search_url(query):
        encoded_query = urllib.parse.quote(query)
        return f"{base_search}?q={encoded_query}&{suffix}"

    topics = {
        "é¦–é ": [
            f"https://trends.google.com/trends/trendingsearches/daily/rss?geo=TW",
            f"https://news.google.com/rss?{suffix}"
        ],
        "æ”¿æ²»": [make_search_url("å°ç£æ”¿æ²» ç«‹æ³•é™¢")],
        "è²¡ç¶“": [make_search_url("å°ç£è‚¡å¸‚ è²¡ç¶“ å°ç©é›»")],
        "ç§‘æŠ€": [make_search_url("å°ç£ç§‘æŠ€ åŠå°é«” AI")],
        "å¨›æ¨‚": [make_search_url("å°ç£å¨›æ¨‚æ–°è ç¶²ç´… è—äºº")],
        "é‹å‹•": [make_search_url("ä¸­è¯è·æ£’ NBA å°ç£é‹å‹•")],
        "åœ‹éš›": [make_search_url("åœ‹éš›æ–°è ç¾åœ‹ ä¸­åœ‹")],
        "å¥åº·": [make_search_url("å¥åº·é†«ç™‚ é£Ÿå®‰ ç–«æƒ…")]
    }
    return topics.get(category, topics["é¦–é "])

# ================= 4. æ ¸å¿ƒåŠŸèƒ½ï¼šAI åˆ†æ (å«é™¤éŒ¯ç´€éŒ„) =================
@st.cache_data(ttl=1800)
def run_analysis(category):
    debug_logs = [] # è¨˜éŒ„ç³»çµ±é‹ä½œéç¨‹
    
    # A. æ¨¡å‹è¨­å®š
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    generation_config = {"temperature": 1, "response_mime_type": "application/json"}
    
    try:
        model = genai.GenerativeModel('gemini-2.5-flash', safety_settings=safety_settings, generation_config=generation_config)
    except:
        model = genai.GenerativeModel('gemini-pro', safety_settings=safety_settings)

    # B. æŠ“å–è³‡æ–™
    urls = get_rss_url(category)
    all_raw_data = []
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    for url in urls:
        try:
            debug_logs.append(f"æ­£åœ¨æŠ“å–: {url}")
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                debug_logs.append(f"âœ… æˆåŠŸï¼ŒæŠ“åˆ° {len(feed.entries)} å‰‡æ–°è")
                
                limit = 25 if category == "é¦–é " else 15
                
                for entry in feed.entries[:limit]:
                    traffic = entry.ht_approx_traffic if hasattr(entry, 'ht_approx_traffic') else "N/A"
                    all_raw_data.append({
                        "title": entry.title,
                        "traffic": traffic,
                        "snippet": entry.summary if hasattr(entry, 'summary') else ""
                    })
            else:
                debug_logs.append(f"âŒ å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
        except Exception as e:
            debug_logs.append(f"âŒ éŒ¯èª¤: {str(e)}")
            continue

    if not all_raw_data:
        return [], debug_logs # å›å‚³ç©ºçš„è³‡æ–™å’ŒéŒ¯èª¤ç´€éŒ„

    # C. AI åˆ†æ
    news_json = json.dumps(all_raw_data, ensure_ascii=False)
    
    if category == "é¦–é ":
        task_desc = "è«‹åˆ—å‡º **15-20 å€‹** å°ç£ç¾åœ¨ **å…¨ç¶²æœ€ç†±é–€ã€æœ€å¤šå…ƒ** çš„è¨è«–è©±é¡Œ (åŒ…å«æ”¿æ²»ã€å¨›æ¨‚ã€ç”Ÿæ´»ç­‰)ã€‚"
    else:
        task_desc = f"è«‹å°ˆæ³¨æ–¼ **{category}** é ˜åŸŸï¼Œåˆ—å‡º **10-15 å€‹** è©²é ˜åŸŸç›®å‰æœ€å—é—œæ³¨çš„è­°é¡Œã€‚"

    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ç£ç¤¾ç¾¤è¶¨å‹¢è§€å¯Ÿå®¶ã€‚è«‹åˆ†æä»¥ä¸‹è³‡æ–™ã€‚
    {task_desc}

    åŸå§‹è³‡æ–™ï¼š
    {news_json}
    
    è¦æ±‚ï¼š
    1. åˆä½µé‡è¤‡çš„äº‹ä»¶ã€‚
    2. æœ‰æµé‡æ•¸æ“š (å¦‚ "50,000+") åˆ†æ•¸çµ¦é«˜ã€‚
    3. ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚
    
    è«‹å›å‚³ JSON Arrayï¼š
    [
      {{
        "id": 1,
        "keyword": "è©±é¡Œé—œéµå­—",
        "category": "åˆ†é¡ (å¦‚: {category})",
        "score": 95,
        "volume_label": "è¨è«–é‡ç´š (å¦‚: 5è¬+ æœå°‹ / ç†±è­°ä¸­)",
        "summary": "ç°¡çŸ­èªªæ˜ã€‚",
        "hashtags": ["#tag1"]
      }}
    ]
    """
    
    try:
        response = model.generate_content(prompt)
        cleaned_text = response.text.replace("```json", "").replace("```", "").strip()
        if not cleaned_text: 
            debug_logs.append("âš ï¸ AI å›å‚³ç©ºç™½å…§å®¹")
            return [], debug_logs
        return json.loads(cleaned_text), debug_logs
    except Exception as e:
        debug_logs.append(f"âš ï¸ AI åˆ†æéŒ¯èª¤: {e}")
        return [], debug_logs

# ================= 5. ä»‹é¢é¡¯ç¤º (UI) =================

# å´é‚Šæ¬„
st.sidebar.title("ğŸ”¥ å°è¦½é¸å–®")
st.sidebar.markdown("è«‹é¸æ“‡æ‚¨æ„Ÿèˆˆè¶£çš„çœ‹ç‰ˆï¼š")

options = ["é¦–é  (å…¨ç¶²ç†±æœ)", "âš–ï¸ æ”¿æ²»", "ğŸ’° è²¡ç¶“", "ğŸ’» ç§‘æŠ€", "ğŸ¿ å¨›æ¨‚", "âš¾ é‹å‹•", "ğŸŒ åœ‹éš›", "ğŸ¥ å¥åº·"]
selection = st.sidebar.radio("Go to", options, label_visibility="collapsed")

category_map = {
    "é¦–é  (å…¨ç¶²ç†±æœ)": "é¦–é ",
    "âš–ï¸ æ”¿æ²»": "æ”¿æ²»",
    "ğŸ’° è²¡ç¶“": "è²¡ç¶“",
    "ğŸ’» ç§‘æŠ€": "ç§‘æŠ€",
    "ğŸ¿ å¨›æ¨‚": "å¨›æ¨‚",
    "âš¾ é‹å‹•": "é‹å‹•",
    "ğŸŒ åœ‹éš›": "åœ‹éš›",
    "ğŸ¥ å¥åº·": "å¥åº·"
}
current_category = category_map[selection]

# æ¨™é¡Œå€
col1, col2 = st.columns([3, 1])
with col1:
    if current_category == "é¦–é ":
        st.title("ğŸ‡¹ğŸ‡¼ å°ç£ç†±é–€è¨è«–")
    else:
        st.title(f"ğŸ‡¹ğŸ‡¼ {current_category}ç†±é–€è¨è«–")
    st.caption(f"å³æ™‚ AI è¼¿æƒ…åˆ†æ | æ›´æ–°: {datetime.now().strftime('%H:%M')}")

with col2:
    if st.button("ğŸ”„ é‡æ–°æ•´ç†"):
        st.cache_data.clear()
        st.rerun()

st.divider()

# åŸ·è¡Œåˆ†æ
with st.spinner(f'ğŸ” æ­£åœ¨æƒæ {current_category} ç‰ˆé¢æ–°èèˆ‡è¶¨å‹¢...'):
    trends, logs = run_analysis(current_category)

if trends:
    # é¡¯ç¤ºçµæœ
    st.markdown("""
    <style>
        a.trend-link { text-decoration: none !important; color: inherit !important; display: block; }
        .trend-row { background-color: white; padding: 15px; border-radius: 10px; margin-bottom: 12px; border: 1px solid #eee; transition: all 0.2s ease; cursor: pointer; }
        .trend-row:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); border-color: #FF4B4B; }
        .rank-num { font-size: 1.5em; font-weight: bold; color: #ccc; width: 40px; text-align: center; }
        .rank-1 { color: #FF4B4B; }
        .rank-2 { color: #FF8F00; }
        .rank-3 { color: #FFC107; }
        .volume-badge { background-color: #ffebee; color: #c62828; padding: 3px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; }
        .category-badge { background-color: #f1f3f4; color: #555; padding: 3px 8px; border-radius: 4px; font-size: 0.8em; }
    </style>
    """, unsafe_allow_html=True)

    if current_category == "é¦–é ":
        st.subheader("ğŸ“Š è©±é¡Œç†±åº¦åˆ†ä½ˆ")
        df = pd.DataFrame(trends)
        if not df.empty:
            st.bar_chart(df.set_index('keyword')['score'], color="#FF4B4B")

    st.subheader(f"ğŸ† {current_category}è©±é¡Œæ’è¡Œæ¦œ")
    
    for i, item in enumerate(trends):
        rank_class = f"rank-{i+1}" if i < 3 else ""
        search_query = urllib.parse.quote(item['keyword'])
        google_url = f"https://www.google.com/search?q={search_query}"
        
        st.markdown(f"""
        <a href="{google_url}" target="_blank" class="trend-link">
            <div class="trend-row">
                <div style="display:flex; align-items:center;">
                    <div class="rank-num {rank_class}">{i+1}</div>
                    <div style="flex-grow:1; padding-left:15px;">
                        <div style="display:flex; justify-content:space-between; align-items:center;">
                            <h3 style="margin:0; font-size:1.2em; color:#333;">{item['keyword']} ğŸ”—</h3>
                            <span class="volume-badge">ğŸ”¥ {item.get('volume_label', 'ç†±è­°ä¸­')}</span>
                        </div>
                        <div style="margin-top:5px; font-size:0.9em; color:#666;">
                            <span class="category-badge">{item.get('category', current_category)}</span>
                            <span style="margin-left:8px;">{item['summary']}</span>
                        </div>
                        <div style="margin-top:8px; font-size:0.85em; color:#888;">
                            {' '.join([f'#{tag}' for tag in item.get('hashtags', [])]).replace('##', '#')}
                        </div>
                    </div>
                </div>
            </div>
        </a>
        """, unsafe_allow_html=True)

else:
    # --- é€™è£¡å°±æ˜¯é—œéµçš„é™¤éŒ¯å€ ---
    st.error("ç›®å‰ç„¡æ³•å–å¾—è³‡æ–™ï¼Œè«‹åƒè€ƒä¸‹æ–¹ç³»çµ±ç´€éŒ„ï¼š")
    with st.expander("ğŸ› ï¸ ç³»çµ±é™¤éŒ¯ç´€éŒ„ (Debug Logs)", expanded=True):
        for log in logs:
            st.write(log)
    st.info("è«‹å°‡ä¸Šè¿°ç´€éŒ„æˆªåœ–å›å ±ï¼Œä»¥ä¾¿ä¿®å¾©ã€‚")
