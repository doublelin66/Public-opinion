import streamlit as st
import google.generativeai as genai
import feedparser
import requests
import json
import pandas as pd
from datetime import datetime, timedelta
import urllib.parse

# ================= 1. åŸºç¤è¨­å®š =================
st.set_page_config(
    page_title="ğŸ‡¹ğŸ‡¼å°ç£ç†±é–€è¨è«–",
    page_icon="ğŸ”¥",
    layout="wide"
)

# ================= 2. å®‰å…¨è®€å–é‡‘é‘° =================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âš ï¸ æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹ç¢ºèª Streamlit Secrets è¨­å®šã€‚")
    st.stop()

genai.configure(api_key=API_KEY)

# ================= 3. å®šç¾©æ–°èä¾†æº (é—œéµä¿®æ­£ï¼šå¼·åˆ¶æ™‚æ•ˆ) =================
def get_rss_url(category):
    base_search = "https://news.google.com/rss/search"
    suffix = "hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    # --- é—œéµä¿®æ­£ï¼šåŠ å…¥æ™‚é–“é™åˆ¶èˆ‡æ’åº ---
    def make_search_url(query):
        # 1. åŠ å…¥ when:2d (éå»48å°æ™‚)ï¼Œç¢ºä¿æ–°èå¤ æ–°ï¼Œä½†åˆä¸æœƒå› ç‚ºå¤ªçŸ­æŠ“ä¸åˆ°è³‡æ–™
        query_with_time = f"{query} when:2d"
        encoded_query = urllib.parse.quote(query_with_time)
        
        # 2. åŠ å…¥ scoring=n (Newest)ï¼Œå¼·åˆ¶ä¾ç…§æ™‚é–“æ’åº
        return f"{base_search}?q={encoded_query}&scoring=n&{suffix}"

    topics = {
        "é¦–é ": [
            # Google Trends æœ¬èº«å°±æ˜¯å³æ™‚çš„ï¼Œä¸ç”¨æ”¹
            f"https://trends.google.com/trends/trendingsearches/daily/rss?geo=TW",
            # ç¶œåˆé ­æ¢å¼·åˆ¶æŠ“æœ€æ–°çš„
            f"https://news.google.com/rss?{suffix}"
        ],
        # å…¶ä»–åˆ†é¡å…¨éƒ¨åŠ ä¸Šæ™‚é–“é™åˆ¶
        "æ”¿æ²»": [make_search_url("å°ç£æ”¿æ²» ç«‹æ³•é™¢ è¡Œæ”¿é™¢")],
        "è²¡ç¶“": [make_search_url("å°ç£è‚¡å¸‚ è²¡ç¶“ å°ç©é›» ç‡Ÿæ”¶")],
        "ç§‘æŠ€": [make_search_url("å°ç£ç§‘æŠ€ åŠå°é«” AI è¼é”")],
        "å¨›æ¨‚": [make_search_url("å°ç£å¨›æ¨‚æ–°è ç¶²ç´… è—äºº ç›´æ’­")],
        "é‹å‹•": [make_search_url("ä¸­è¯è·æ£’ NBA å°ç£é‹å‹•")],
        "åœ‹éš›": [make_search_url("åœ‹éš›æ–°è ç¾åœ‹ æ—¥æœ¬ ä¸­åœ‹")],
        "å¥åº·": [make_search_url("å¥åº·é†«ç™‚ é£Ÿå®‰ æµæ„Ÿ è…¸ç—…æ¯’")]
    }
    return topics.get(category, topics["é¦–é "])

# ================= 4. æ ¸å¿ƒåŠŸèƒ½ï¼šAI åˆ†æ =================
@st.cache_data(ttl=900) # æ”¹ç‚º 15 åˆ†é˜æ›´æ–°ä¸€æ¬¡ï¼Œè³‡è¨Šæ›´æ–°é®®
def run_analysis(category):
    debug_logs = []
    
    # A. æ¨¡å‹è¨­å®š
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    generation_config = {"temperature": 1, "response_mime_type": "application/json"}
    
    try:
        model = genai.GenerativeModel('gemini-2.5-flash', safety_settings=safety_settings, generation_config=generation_config)
    except:
        model = genai.GenerativeModel('gemini-pro', safety_settings=safety_settings)

    # B. æŠ“å–è³‡æ–™
    urls = get_rss_url(category)
    all_raw_data = []
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    for url in urls:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                
                limit = 30 if category == "é¦–é " else 20 # ç¨å¾®å¢åŠ æŠ“å–é‡ï¼Œç¢ºä¿éæ¿¾å¾Œé‚„æœ‰å‰©
                
                for entry in feed.entries[:limit]:
                    traffic = entry.ht_approx_traffic if hasattr(entry, 'ht_approx_traffic') else "N/A"
                    # é¡å¤–æª¢æŸ¥ç™¼å¸ƒæ™‚é–“ï¼Œå¦‚æœæ˜¯å¤ªèˆŠçš„(è¶…é3å¤©)ç›´æ¥ä¸Ÿæ‰
                    all_raw_data.append({
                        "title": entry.title,
                        "pubDate": entry.published if hasattr(entry, 'published') else "",
                        "traffic": traffic,
                        "snippet": entry.summary if hasattr(entry, 'summary') else ""
                    })
        except Exception as e:
            debug_logs.append(str(e))
            continue

    if not all_raw_data:
        return [], debug_logs

    # C. AI åˆ†æ
    news_json = json.dumps(all_raw_data, ensure_ascii=False)
    
    if category == "é¦–é ":
        task_desc = "è«‹åˆ—å‡º **15-20 å€‹** å°ç£ç¾åœ¨ **å…¨ç¶²æœ€ç†±é–€ã€æœ€æ–°** çš„è¨è«–è©±é¡Œã€‚"
    else:
        task_desc = f"è«‹å°ˆæ³¨æ–¼ **{category}** é ˜åŸŸï¼Œåˆ—å‡º **10-15 å€‹** è©²é ˜åŸŸ **é€™å…©å¤©å…§** æœ€å—é—œæ³¨çš„è­°é¡Œã€‚"

    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ç£ç¤¾ç¾¤è¶¨å‹¢è§€å¯Ÿå®¶ã€‚è«‹åˆ†æä»¥ä¸‹è³‡æ–™ï¼Œæ‰¾å‡ºã€Œç¾åœ¨é€²è¡Œå¼ã€çš„ç†±é–€è©±é¡Œã€‚
    {task_desc}

    åŸå§‹è³‡æ–™ï¼š
    {news_json}
    
    è¦æ±‚ï¼š
    1. **æ™‚æ•ˆå„ªå…ˆ**ï¼šè«‹å¿½ç•¥èˆŠèï¼Œåªå°ˆæ³¨åœ¨æœ€è¿‘ç™¼ç”Ÿçš„äº‹ä»¶ã€‚
    2. åˆä½µé‡è¤‡çš„äº‹ä»¶ã€‚
    3. æœ‰æµé‡æ•¸æ“š (å¦‚ "50,000+") åˆ†æ•¸çµ¦é«˜ã€‚
    4. ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚
    
    è«‹å›å‚³ JSON Arrayï¼š
    [
      {{
        "id": 1,
        "keyword": "è©±é¡Œé—œéµå­—",
        "category": "åˆ†é¡ (å¦‚: {category})",
        "score": 95,
        "volume_label": "è¨è«–é‡ç´š (å¦‚: 5è¬+ æœå°‹ / ç†±è­°ä¸­)",
        "summary": "ç°¡çŸ­èªªæ˜ã€‚",
        "hashtags": ["#tag1"]
      }}
    ]
    """
    
    try:
        response = model.generate_content(prompt)
        cleaned_text = response.text.replace("```json", "").replace("```", "").strip()
        if not cleaned_text: return [], debug_logs
        return json.loads(cleaned_text), debug_logs
    except Exception as e:
        return [], [str(e)]

# ================= 5. ä»‹é¢é¡¯ç¤º (UI) =================

# å´é‚Šæ¬„
st.sidebar.title("ğŸ”¥ å°è¦½é¸å–®")
st.sidebar.markdown("è«‹é¸æ“‡æ‚¨æ„Ÿèˆˆè¶£çš„çœ‹ç‰ˆï¼š")

options = ["é¦–é  (å…¨ç¶²ç†±æœ)", "âš–ï¸ æ”¿æ²»", "ğŸ’° è²¡ç¶“", "ğŸ’» ç§‘æŠ€", "ğŸ¿ å¨›æ¨‚", "âš¾ é‹å‹•", "ğŸŒ åœ‹éš›", "ğŸ¥ å¥åº·"]
selection = st.sidebar.radio("Go to", options, label_visibility="collapsed")

category_map = {
    "é¦–é  (å…¨ç¶²ç†±æœ)": "é¦–é ",
    "âš–ï¸ æ”¿æ²»": "æ”¿æ²»",
    "ğŸ’° è²¡ç¶“": "è²¡ç¶“",
    "ğŸ’» ç§‘æŠ€": "ç§‘æŠ€",
    "ğŸ¿ å¨›æ¨‚": "å¨›æ¨‚",
    "âš¾ é‹å‹•": "é‹å‹•",
    "ğŸŒ åœ‹éš›": "åœ‹éš›",
    "ğŸ¥ å¥åº·": "å¥åº·"
}
current_category = category_map[selection]

# æ¨™é¡Œå€
col1, col2 = st.columns([3, 1])
with col1:
    if current_category == "é¦–é ":
        st.title("ğŸ‡¹ğŸ‡¼ å°ç£ç†±é–€è¨è«–")
    else:
        st.title(f"ğŸ‡¹ğŸ‡¼ {current_category}ç†±é–€è¨è«–")
    st.caption(f"å³æ™‚ AI è¼¿æƒ…åˆ†æ | è³‡æ–™ç¯„åœ: 48å°æ™‚å…§ | æ›´æ–°: {datetime.now().strftime('%H:%M')}")

with col2:
    if st.button("ğŸ”„ é‡æ–°æ•´ç†"):
        st.cache_data.clear()
        st.rerun()

st.divider()

# åŸ·è¡Œåˆ†æ
with st.spinner(f'ğŸ” æ­£åœ¨æƒæ {current_category} ç‰ˆé¢ã€Œæœ€æ–°ã€æ–°èèˆ‡è¶¨å‹¢...'):
    trends, logs = run_analysis(current_category)

if trends:
    st.markdown("""
    <style>
        a.trend-link { text-decoration: none !important; color: inherit !important; display: block; }
        .trend-row { background-color: white; padding: 15px; border-radius: 10px; margin-bottom: 12px; border: 1px solid #eee; transition: all 0.2s ease; cursor: pointer; }
        .trend-row:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); border-color: #FF4B4B; }
        .rank-num { font-size: 1.5em; font-weight: bold; color: #ccc; width: 40px; text-align: center; }
        .rank-1 { color: #FF4B4B; }
        .rank-2 { color: #FF8F00; }
        .rank-3 { color: #FFC107; }
        .volume-badge { background-color: #ffebee; color: #c62828; padding: 3px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; }
        .category-badge { background-color: #f1f3f4; color: #555; padding: 3px 8px; border-radius: 4px; font-size: 0.8em; }
    </style>
    """, unsafe_allow_html=True)

    if current_category == "é¦–é ":
        st.subheader("ğŸ“Š è©±é¡Œç†±åº¦åˆ†ä½ˆ")
        df = pd.DataFrame(trends)
        if not df.empty:
            st.bar_chart(df.set_index('keyword')['score'], color="#FF4B4B")

    st.subheader(f"ğŸ† {current_category}è©±é¡Œæ’è¡Œæ¦œ")
    
    for i, item in enumerate(trends):
        rank_class = f"rank-{i+1}" if i < 3 else ""
        search_query = urllib.parse.quote(item['keyword'])
        google_url = f"https://www.google.com/search?q={search_query}" # é»æ“Šå¾Œå» Google æœå°‹
        
        st.markdown(f"""
        <a href="{google_url}" target="_blank" class="trend-link">
            <div class="trend-row">
                <div style="display:flex; align-items:center;">
                    <div class="rank-num {rank_class}">{i+1}</div>
                    <div style="flex-grow:1; padding-left:15px;">
                        <div style="display:flex; justify-content:space-between; align-items:center;">
                            <h3 style="margin:0; font-size:1.2em; color:#333;">{item['keyword']} ğŸ”—</h3>
                            <span class="volume-badge">ğŸ”¥ {item.get('volume_label', 'ç†±è­°ä¸­')}</span>
                        </div>
                        <div style="margin-top:5px; font-size:0.9em; color:#666;">
                            <span class="category-badge">{item.get('category', current_category)}</span>
                            <span style="margin-left:8px;">{item['summary']}</span>
                        </div>
                        <div style="margin-top:8px; font-size:0.85em; color:#888;">
                            {' '.join([f'#{tag}' for tag in item.get('hashtags', [])]).replace('##', '#')}
                        </div>
                    </div>
                </div>
            </div>
        </a>
        """, unsafe_allow_html=True)

else:
    st.error("ç›®å‰ç„¡æ³•å–å¾—è³‡æ–™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
