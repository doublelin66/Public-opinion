import streamlit as st
import google.generativeai as genai
import feedparser
import requests
import json
import pandas as pd
from datetime import datetime, timedelta
import urllib.parse
import random
import time

# ================= 1. åŸºç¤è¨­å®š =================
st.set_page_config(
    page_title="ğŸ‡¹ğŸ‡¼å°ç£ç†±é–€è¨è«–",
    page_icon="ğŸ”¥",
    layout="wide"
)

# ================= 2. å®‰å…¨è®€å–é‡‘é‘° =================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âš ï¸ æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹ç¢ºèª Streamlit Secrets è¨­å®šã€‚")
    st.stop()

genai.configure(api_key=API_KEY)

# ================= 3. å®šç¾©æ–°èä¾†æº =================
def get_rss_urls(category):
    base_search = "https://news.google.com/rss/search"
    base_topic = "https://news.google.com/rss/topics"
    suffix = "hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    def make_search_url(query):
        # åŠ å…¥ when:2d ç¢ºä¿æ–°èæ–°é®®åº¦
        query_with_time = f"{query} when:2d"
        encoded_query = urllib.parse.quote(query_with_time)
        return f"{base_search}?q={encoded_query}&scoring=n&{suffix}"

    topic_ids = {
        "æ”¿æ²»": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0FBUWlHZ0pKVERNU0FBUW",
        "è²¡ç¶“": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0FBUWlHZ0pKVERNU0FBUW",
        "ç§‘æŠ€": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRGRqTVhZU0FBUWlHZ0pKVERNU0FBUW",
        "å¨›æ¨‚": "CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0FBUWlHZ0pKVERNU0FBUW",
        "é‹å‹•": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0FBUWlHZ0pKVERNU0FBUW",
        "åœ‹éš›": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0FBUWlHZ0pKVERNU0FBUW",
        "å¥åº·": "CAAqIaHZBAgESHgQAlIICgYI1p2w8wIw8MuzAzC4rYoD"
    }

    primary_url = ""
    if category == "é¦–é ":
        return [
            f"https://trends.google.com/trends/trendingsearches/daily/rss?geo=TW",
            f"https://news.google.com/rss?{suffix}"
        ]
    elif category == "æ”¿æ²»": primary_url = make_search_url("å°ç£æ”¿æ²» ç«‹æ³•é™¢ è¡Œæ”¿é™¢")
    elif category == "è²¡ç¶“": primary_url = make_search_url("å°ç£è‚¡å¸‚ è²¡ç¶“ å°ç©é›» ç‡Ÿæ”¶")
    elif category == "ç§‘æŠ€": primary_url = make_search_url("å°ç£ç§‘æŠ€ åŠå°é«” AI è¼é”")
    elif category == "å¨›æ¨‚": primary_url = make_search_url("å°ç£å¨›æ¨‚æ–°è ç¶²ç´… è—äºº ç›´æ’­")
    elif category == "é‹å‹•": primary_url = make_search_url("ä¸­è¯è·æ£’ NBA å°ç£é‹å‹•")
    elif category == "åœ‹éš›": primary_url = make_search_url("åœ‹éš›æ–°è ç¾åœ‹ æ—¥æœ¬ ä¸­åœ‹")
    elif category == "å¥åº·": primary_url = make_search_url("å¥åº·é†«ç™‚ é£Ÿå®‰ æµæ„Ÿ è…¸ç—…æ¯’")

    backup_url = ""
    if category in topic_ids:
        backup_url = f"{base_topic}/{topic_ids[category]}?{suffix}"
    else:
        backup_url = f"https://news.google.com/rss?{suffix}"

    return [primary_url, backup_url]

# ================= 4. æ ¸å¿ƒåŠŸèƒ½ï¼šAI åˆ†æ (å…¨æ¨¡å‹è¼ªæ›¿) =================
@st.cache_data(ttl=1800) 
def run_analysis(category):
    debug_logs = []
    
    # æŠ“å–è³‡æ–™
    target_urls = get_rss_urls(category)
    all_raw_data = []
    
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    ]
    
    headers = {
        "User-Agent": random.choice(user_agents),
        "Accept": "application/rss+xml, application/xml, text/xml, */*",
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
    }
    cookies = {"CONSENT": "YES+"} 

    success_count = 0
    for url in target_urls:
        if success_count > 0 and category != "é¦–é ": break 
        try:
            time.sleep(random.uniform(0.1, 0.5))
            response = requests.get(url, headers=headers, cookies=cookies, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                if len(feed.entries) > 0:
                    limit = 25 if category == "é¦–é " else 15
                    for entry in feed.entries[:limit]:
                        traffic = entry.ht_approx_traffic if hasattr(entry, 'ht_approx_traffic') else "N/A"
                        all_raw_data.append({
                            "title": entry.title,
                            "traffic": traffic,
                            "snippet": entry.summary if hasattr(entry, 'summary') else ""
                        })
                    success_count += 1
        except Exception as e:
            debug_logs.append(f"é€£ç·šéŒ¯èª¤: {str(e)}")
            continue

    if not all_raw_data:
        return [], debug_logs

    # --- AI åˆ†æ ---
    news_json = json.dumps(all_raw_data, ensure_ascii=False)
    
    if category == "é¦–é ":
        task_desc = "è«‹åˆ—å‡º **15-20 å€‹** å°ç£ç¾åœ¨ **å…¨ç¶²æœ€ç†±é–€ã€æœ€æ–°** çš„è¨è«–è©±é¡Œã€‚"
    else:
        task_desc = f"è«‹å°ˆæ³¨æ–¼ **{category}** é ˜åŸŸï¼Œåˆ—å‡º **10-15 å€‹** è©²é ˜åŸŸ **é€™å…©å¤©å…§** æœ€å—é—œæ³¨çš„è­°é¡Œã€‚"

    json_example = f"""
    [
      {{
        "id": 1,
        "keyword": "è©±é¡Œé—œéµå­—",
        "category": "åˆ†é¡ (å¦‚: {category})",
        "score": 95,
        "volume_label": "è¨è«–é‡ç´š (å¦‚: 5è¬+ æœå°‹ / ç†±è­°ä¸­)",
        "summary": "ç°¡çŸ­èªªæ˜",
        "hashtags": ["#tag1"]
      }}
    ]
    """

    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ç£ç¤¾ç¾¤è¶¨å‹¢è§€å¯Ÿå®¶ã€‚è«‹åˆ†æä»¥ä¸‹è³‡æ–™ï¼Œæ‰¾å‡ºã€Œç¾åœ¨é€²è¡Œå¼ã€çš„ç†±é–€è©±é¡Œã€‚
    {task_desc}

    åŸå§‹è³‡æ–™ï¼š
    {news_json}
    
    è¦æ±‚ï¼š
    1. **æ™‚æ•ˆå„ªå…ˆ**ï¼šè«‹å¿½ç•¥èˆŠèã€‚
    2. åˆä½µé‡è¤‡çš„äº‹ä»¶ã€‚
    3. æœ‰æµé‡æ•¸æ“šåˆ†æ•¸çµ¦é«˜ã€‚
    4. ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚
    
    è«‹å›å‚³ JSON Arrayï¼š
    {json_example}
    """

    # --- é—œéµä¿®æ­£ï¼šè¶…ç´šæ•£å½ˆæ§æ¨¡å¼ ---
    # é€™è£¡åˆ—å‡ºäº† Google ç›®å‰æ‰€æœ‰é–‹æ”¾çš„å…è²»æ¨¡å‹åç¨±
    # åªè¦å…¶ä¸­æœ‰ä¸€å€‹èƒ½é€šï¼Œæ‚¨çš„ç¶²ç«™å°±æœƒæ´»è‘—
    models_to_try = [
        'gemini-2.0-flash',       # æœ€å¼·ï¼Œä½†å®¹æ˜“è¢«æ“‹
        'gemini-1.5-flash',       # é¡åº¦æœ€é«˜ (æ¯å¤©1500æ¬¡)ï¼Œæœ€ç©©
        'gemini-1.5-flash-latest',# 1.5 çš„æœ€æ–°ç‰ˆè®Šé«”
        'gemini-1.5-flash-001',   # 1.5 çš„èˆŠç‰ˆè®Šé«” (æœ‰æ™‚å€™ 404 æ˜¯å› ç‚ºæ²’åŠ ç‰ˆè™Ÿ)
        'gemini-1.5-flash-002',   # 1.5 çš„æ›´æ–°ç‰ˆè®Šé«”
        'gemini-1.5-flash-8b',    # 8B ç‰ˆ (è¼•é‡ç´šï¼Œé¡åº¦é€šå¸¸ç¨ç«‹è¨ˆç®—)
        'gemini-2.0-flash-exp',   # 2.0 å¯¦é©—ç‰ˆ
        'gemini-2.5-flash'        # 2.5 é è¦½ç‰ˆ
    ]

    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    generation_config = {"temperature": 1, "response_mime_type": "application/json"}

    for model_name in models_to_try:
        try:
            # é€™è£¡ä¸å° Log äº†ï¼Œä»¥å…åš‡åˆ°ä½¿ç”¨è€…ï¼Œé»˜é»˜å˜—è©¦å°±å¥½
            model = genai.GenerativeModel(model_name, safety_settings=safety_settings, generation_config=generation_config)
            
            response = model.generate_content(prompt)
            cleaned_text = response.text.replace("```json", "").replace("```", "").strip()
            
            if cleaned_text:
                return json.loads(cleaned_text), debug_logs
                
        except Exception as e:
            # å¤±æ•—å°±æ›ä¸‹ä¸€å€‹ï¼Œä¸è¦åœ
            debug_logs.append(f"âŒ {model_name} å¤±æ•—ï¼Œå˜—è©¦ä¸‹ä¸€å€‹...")
            time.sleep(0.5)
            continue
            
    return [], debug_logs

# ================= 5. ä»‹é¢é¡¯ç¤º (UI) =================

st.sidebar.title("ğŸ”¥ å°è¦½é¸å–®")
st.sidebar.markdown("è«‹é¸æ“‡æ‚¨æ„Ÿèˆˆè¶£çš„çœ‹ç‰ˆï¼š")

options = ["é¦–é  (å…¨ç¶²ç†±æœ)", "âš–ï¸ æ”¿æ²»", "ğŸ’° è²¡ç¶“", "ğŸ’» ç§‘æŠ€", "ğŸ¿ å¨›æ¨‚", "âš¾ é‹å‹•", "ğŸŒ åœ‹éš›", "ğŸ¥ å¥åº·"]
selection = st.sidebar.radio("Go to", options, label_visibility="collapsed")

category_map = {
    "é¦–é  (å…¨ç¶²ç†±æœ)": "é¦–é ", "âš–ï¸ æ”¿æ²»": "æ”¿æ²»", "ğŸ’° è²¡ç¶“": "è²¡ç¶“",
    "ğŸ’» ç§‘æŠ€": "ç§‘æŠ€", "ğŸ¿ å¨›æ¨‚": "å¨›æ¨‚", "âš¾ é‹å‹•": "é‹å‹•",
    "ğŸŒ åœ‹éš›": "åœ‹éš›", "ğŸ¥ å¥åº·": "å¥åº·"
}
current_category = category_map[selection]

col1, col2 = st.columns([3, 1])
with col1:
    if current_category == "é¦–é ":
        st.title("ğŸ‡¹ğŸ‡¼ å°ç£ç†±é–€è¨è«–")
    else:
        st.title(f"ğŸ‡¹ğŸ‡¼ {current_category}ç†±é–€è¨è«–")
    st.caption(f"å³æ™‚ AI è¼¿æƒ…åˆ†æ | è³‡æ–™ç¯„åœ: 48å°æ™‚å…§ | æ›´æ–°: {datetime.now().strftime('%H:%M')}")

with col2:
    if st.button("ğŸ”„ é‡æ–°æ•´ç†"):
        st.rerun()

st.divider()

with st.spinner(f'ğŸ” æ­£åœ¨æƒæ {current_category} ç‰ˆé¢ï¼Œä¸¦å˜—è©¦é€£æ¥æœ€ä½³ AI æ¨¡å‹...'):
    trends, logs = run_analysis(current_category)

if trends:
    st.markdown("""
    <style>
        a.trend-link { text-decoration: none !important; color: inherit !important; display: block; }
        .trend-row { background-color: white; padding: 15px; border-radius: 10px; margin-bottom: 12px; border: 1px solid #eee; transition: all 0.2s ease; cursor: pointer; }
        .trend-row:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); border-color: #FF4B4B; }
        .rank-num { font-size: 1.5em; font-weight: bold; color: #ccc; width: 40px; text-align: center; }
        .rank-1 { color: #FF4B4B; }
        .rank-2 { color: #FF8F00; }
        .rank-3 { color: #FFC107; }
        .volume-badge { background-color: #ffebee; color: #c62828; padding: 3px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; }
        .category-badge { background-color: #f1f3f4; color: #555; padding: 3px 8px; border-radius: 4px; font-size: 0.8em; }
    </style>
    """, unsafe_allow_html=True)

    if current_category == "é¦–é ":
        st.subheader("ğŸ“Š è©±é¡Œç†±åº¦åˆ†ä½ˆ")
        df = pd.DataFrame(trends)
        if not df.empty:
            st.bar_chart(df.set_index('keyword')['score'], color="#FF4B4B")

    st.subheader(f"ğŸ† {current_category}è©±é¡Œæ’è¡Œæ¦œ")
    
    for i, item in enumerate(trends):
        rank_class = f"rank-{i+1}" if i < 3 else ""
        search_query = urllib.parse.quote(item['keyword'])
        google_url = f"https://www.google.com/search?q={search_query}"
        
        st.markdown(f"""
        <a href="{google_url}" target="_blank" class="trend-link">
            <div class="trend-row">
                <div style="display:flex; align-items:center;">
                    <div class="rank-num {rank_class}">{i+1}</div>
                    <div style="flex-grow:1; padding-left:15px;">
                        <div style="display:flex; justify-content:space-between; align-items:center;">
                            <h3 style="margin:0; font-size:1.2em; color:#333;">{item['keyword']} ğŸ”—</h3>
                            <span class="volume-badge">ğŸ”¥ {item.get('volume_label', 'ç†±è­°ä¸­')}</span>
                        </div>
                        <div style="margin-top:5px; font-size:0.9em; color:#666;">
                            <span class="category-badge">{item.get('category', current_category)}</span>
                            <span style="margin-left:8px;">{item['summary']}</span>
                        </div>
                        <div style="margin-top:8px; font-size:0.85em; color:#888;">
                            {' '.join([f'#{tag}' for tag in item.get('hashtags', [])]).replace('##', '#')}
                        </div>
                    </div>
                </div>
            </div>
        </a>
        """, unsafe_allow_html=True)

else:
    st.error("ç›®å‰æµé‡éå¤§ï¼Œè³‡æ–™æš«æ™‚ç„¡æ³•è®€å–ã€‚")
    with st.expander("ğŸ› ï¸ ç³»çµ±è¨ºæ–·å ±å‘Š", expanded=True):
        st.write("å˜—è©¦é€£ç·šç´€éŒ„ï¼š")
        for log in logs:
            st.write(log)
