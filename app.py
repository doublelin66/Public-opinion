import streamlit as st
import google.generativeai as genai
import feedparser
import requests
import json
import pandas as pd
from datetime import datetime, timedelta
import urllib.parse
import random

# ================= 1. åŸºç¤è¨­å®š =================
st.set_page_config(
    page_title="ğŸ‡¹ğŸ‡¼å°ç£ç†±é–€è¨è«–",
    page_icon="ğŸ”¥",
    layout="wide"
)

# ================= 2. å®‰å…¨è®€å–é‡‘é‘° =================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âš ï¸ æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹ç¢ºèª Streamlit Secrets è¨­å®šã€‚")
    st.stop()

genai.configure(api_key=API_KEY)

# ================= 3. å®šç¾©æ–°èä¾†æº =================
def get_rss_url(category):
    base_search = "https://news.google.com/rss/search"
    suffix = "hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    def make_search_url(query):
        # åŠ å…¥ when:2d ç¢ºä¿æ–°èæ–°é®®åº¦
        query_with_time = f"{query} when:2d"
        encoded_query = urllib.parse.quote(query_with_time)
        return f"{base_search}?q={encoded_query}&scoring=n&{suffix}"

    topics = {
        "é¦–é ": [
            f"https://trends.google.com/trends/trendingsearches/daily/rss?geo=TW",
            f"https://news.google.com/rss?{suffix}"
        ],
        "æ”¿æ²»": [make_search_url("å°ç£æ”¿æ²» ç«‹æ³•é™¢ è¡Œæ”¿é™¢")],
        "è²¡ç¶“": [make_search_url("å°ç£è‚¡å¸‚ è²¡ç¶“ å°ç©é›» ç‡Ÿæ”¶")],
        "ç§‘æŠ€": [make_search_url("å°ç£ç§‘æŠ€ åŠå°é«” AI è¼é”")],
        "å¨›æ¨‚": [make_search_url("å°ç£å¨›æ¨‚æ–°è ç¶²ç´… è—äºº ç›´æ’­")],
        "é‹å‹•": [make_search_url("ä¸­è¯è·æ£’ NBA å°ç£é‹å‹•")],
        "åœ‹éš›": [make_search_url("åœ‹éš›æ–°è ç¾åœ‹ æ—¥æœ¬ ä¸­åœ‹")],
        "å¥åº·": [make_search_url("å¥åº·é†«ç™‚ é£Ÿå®‰ æµæ„Ÿ è…¸ç—…æ¯’")]
    }
    return topics.get(category, topics["é¦–é "])

# ================= 4. æ ¸å¿ƒåŠŸèƒ½ï¼šAI åˆ†æ =================
# å¿«å–è¨­å®šï¼š30åˆ†é˜å…§ä¸é‡æ–°æŠ“å–ï¼Œé¿å…è¢«æ“‹
@st.cache_data(ttl=1800) 
def run_analysis(category):
    debug_logs = []
    
    # A. æ¨¡å‹è¨­å®š
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    generation_config = {"temperature": 1, "response_mime_type": "application/json"}
    
    try:
        model = genai.GenerativeModel('gemini-2.5-flash', safety_settings=safety_settings, generation_config=generation_config)
    except:
        model = genai.GenerativeModel('gemini-pro', safety_settings=safety_settings)

    # B. æŠ“å–è³‡æ–™
    urls = get_rss_url(category)
    all_raw_data = []
    
    # å½è£ Headers
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://www.google.com/"
    }
    cookies = {"CONSENT": "YES+"} 

    for url in urls:
        try:
            response = requests.get(url, headers=headers, cookies=cookies, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                limit = 30 if category == "é¦–é " else 20
                
                for entry in feed.entries[:limit]:
                    traffic = entry.ht_approx_traffic if hasattr(entry, 'ht_approx_traffic') else "N/A"
                    all_raw_data.append({
                        "title": entry.title,
                        "traffic": traffic,
                        "snippet": entry.summary if hasattr(entry, 'summary') else ""
                    })
            else:
                debug_logs.append(f"HTTP Error: {response.status_code} at {url}")
        except Exception as e:
            debug_logs.append(str(e))
            continue

    if not all_raw_data:
        return [], debug_logs

    # C. AI åˆ†æ (ä¿®æ­£èªæ³•éŒ¯èª¤é¢¨éšª)
    news_json = json.dumps(all_raw_data, ensure_ascii=False)
    
    if category == "é¦–é ":
        task_desc = "è«‹åˆ—å‡º **15-20 å€‹** å°ç£ç¾åœ¨ **å…¨ç¶²æœ€ç†±é–€ã€æœ€æ–°** çš„è¨è«–è©±é¡Œã€‚"
    else:
        task_desc = f"è«‹å°ˆæ³¨æ–¼ **{category}** é ˜åŸŸï¼Œåˆ—å‡º **10-15 å€‹** è©²é ˜åŸŸ **é€™å…©å¤©å…§** æœ€å—é—œæ³¨çš„è­°é¡Œã€‚"

    # --- é€™è£¡å°‡ JSON ç¯„ä¾‹ç¨ç«‹å‡ºä¾†ï¼Œé¿å… f-string èªæ³•éŒ¯èª¤ ---
    json_example = f"""
    [
      {{
        "id": 1,
        "keyword": "è©±é¡Œé—œéµå­—",
        "category": "åˆ†é¡ (å¦‚: {category})",
        "score": 95,
        "volume_label": "è¨è«–é‡ç´š (å¦‚: 5è¬+ æœå°‹ / ç†±è­°ä¸­)",
        "summary": "ç°¡çŸ­èªªæ˜",
        "hashtags": ["#tag1"]
      }}
    ]
    """

    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ç£ç¤¾ç¾¤è¶¨å‹¢è§€å¯Ÿå®¶ã€‚è«‹åˆ†æä»¥ä¸‹è³‡æ–™ï¼Œæ‰¾å‡ºã€Œç¾åœ¨é€²è¡Œå¼ã€çš„ç†±é–€è©±é¡Œã€‚
    {task_desc}

    åŸå§‹è³‡æ–™ï¼š
    {news_json}
    
    è¦æ±‚ï¼š
    1. **æ™‚æ•ˆå„ªå…ˆ**ï¼šè«‹å¿½ç•¥èˆŠèï¼Œåªå°ˆæ³¨åœ¨æœ€è¿‘ç™¼ç”Ÿçš„äº‹ä»¶ã€‚
    2. åˆä½µé‡è¤‡çš„äº‹ä»¶ã€‚
    3. æœ‰æµé‡æ•¸æ“š (å¦‚ "50,000+") åˆ†æ•¸çµ¦é«˜ã€‚
    4. ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚
    
    è«‹å›å‚³ JSON Arrayï¼š
    {json_example}
    """
    
    try:
        response = model.generate_content(prompt)
        cleaned_text = response.text.replace("```json", "").replace("```", "").strip()
        if not cleaned_text: return [], debug_logs
        return json.loads(cleaned_text), debug_logs
    except Exception as e:
        return [], [str(e)]

# ================= 5. ä»‹é¢é¡¯ç¤º (UI) =================

st.sidebar.title("ğŸ”¥ å°è¦½é¸å–®")
st.sidebar.markdown("è«‹é¸æ“‡æ‚¨æ„Ÿèˆˆè¶£çš„çœ‹ç‰ˆï¼š")

options = ["é¦–é  (å…¨ç¶²ç†±æœ)", "âš–ï¸ æ”¿æ²»", "ğŸ’° è²¡ç¶“", "ğŸ’» ç§‘æŠ€", "ğŸ¿ å¨›æ¨‚", "âš¾ é‹å‹•", "ğŸŒ åœ‹éš›", "ğŸ¥ å¥åº·"]
selection = st.sidebar.radio("Go to", options, label_visibility="collapsed")

category_map = {
    "é¦–é  (å…¨ç¶²ç†±æœ)": "é¦–é ", "âš–ï¸ æ”¿æ²»": "æ”¿æ²»", "ğŸ’° è²¡ç¶“": "è²¡ç¶“",
    "ğŸ’» ç§‘æŠ€": "ç§‘æŠ€", "ğŸ¿ å¨›æ¨‚": "å¨›æ¨‚", "âš¾ é‹å‹•": "é‹å‹•",
    "ğŸŒ åœ‹éš›": "åœ‹éš›", "ğŸ¥ å¥åº·": "å¥åº·"
}
current_category = category_map[selection]

col1, col2 = st.columns([3, 1])
with col1:
    if current_category == "é¦–é ":
        st.title("ğŸ‡¹ğŸ‡¼ å°ç£ç†±é–€è¨è«–")
    else:
        st.title(f"ğŸ‡¹ğŸ‡¼ {current_category}ç†±é–€è¨è«–")
    st.caption(f"å³æ™‚ AI è¼¿æƒ…åˆ†æ | è³‡æ–™ç¯„åœ: 48å°æ™‚å…§ | æ›´æ–°: {datetime.now().strftime('%H:%M')}")

with col2:
    if st.button("ğŸ”„ é‡æ–°æ•´ç†"):
        st.rerun()

st.divider()

with st.spinner(f'ğŸ” æ­£åœ¨æƒæ {current_category} ç‰ˆé¢ã€Œæœ€æ–°ã€æ–°èèˆ‡è¶¨å‹¢...'):
    trends, logs = run_analysis(current_category)

if trends:
    st.markdown("""
    <style>
        a.trend-link { text-decoration: none !important; color: inherit !important; display: block; }
        .trend-row { background-color: white; padding: 15px; border-radius: 10px; margin-bottom: 12px; border: 1px solid #eee; transition: all 0.2s ease; cursor: pointer; }
        .trend-row:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); border-color: #FF4B4B; }
        .rank-num { font-size: 1.5em; font-weight: bold; color: #ccc; width: 40px; text-align: center; }
        .rank-1 { color: #FF4B4B; }
        .rank-2 { color: #FF8F00; }
        .rank-3 { color: #FFC107; }
        .volume-badge { background-color: #ffebee; color: #c62828; padding: 3px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; }
        .category-badge { background-color: #f1f3f4; color: #555; padding: 3px 8px; border-radius: 4px; font-size: 0.8em; }
    </style>
    """, unsafe_allow_html=True)

    if current_category == "é¦–é ":
        st.subheader("ğŸ“Š è©±é¡Œç†±åº¦åˆ†ä½ˆ")
        df = pd.DataFrame(trends)
        if not df.empty:
            st.bar_chart(df.set_index('keyword')['score'], color="#FF4B4B")

    st.subheader(f"ğŸ† {current_category}è©±é¡Œæ’è¡Œæ¦œ")
    
    for i, item in enumerate(trends):
        rank_class = f"rank-{i+1}" if i < 3 else ""
        search_query = urllib.parse.quote(item['keyword'])
        google_url = f"https://www.google.com/search?q={search_query}"
        
        st.markdown(f"""
        <a href="{google_url}" target="_blank" class="trend-link">
            <div class="trend-row">
                <div style="display:flex; align-items:center;">
                    <div class="rank-num {rank_class}">{i+1}</div>
                    <div style="flex-grow:1; padding-left:15px;">
                        <div style="display:flex; justify-content:space-between; align-items:center;">
                            <h3 style="margin:0; font-size:1.2em; color:#333;">{item['keyword']} ğŸ”—</h3>
                            <span class="volume-badge">ğŸ”¥ {item.get('volume_label', 'ç†±è­°ä¸­')}</span>
                        </div>
                        <div style="margin-top:5px; font-size:0.9em; color:#666;">
                            <span class="category-badge">{item.get('category', current_category)}</span>
                            <span style="margin-left:8px;">{item['summary']}</span>
                        </div>
                        <div style="margin-top:8px; font-size:0.85em; color:#888;">
                            {' '.join([f'#{tag}' for tag in item.get('hashtags', [])]).replace('##', '#')}
                        </div>
                    </div>
                </div>
            </div>
        </a>
        """, unsafe_allow_html=True)

else:
    st.error("ç›®å‰æµé‡éå¤§ï¼Œè³‡æ–™æš«æ™‚ç„¡æ³•è®€å–ã€‚")
    st.info("ğŸ’¡ ç³»çµ±æç¤ºï¼šé€™å¯èƒ½æ˜¯å› ç‚º Google æ–°èæš«æ™‚é™åˆ¶äº†é€£ç·šï¼Œè«‹é 10-15 åˆ†é˜å¾Œå†å›ä¾†è©¦è©¦çœ‹ï¼Œç„¡éœ€ä¸€ç›´æŒ‰é‡æ–°æ•´ç†ã€‚")
