import streamlit as st
import google.generativeai as genai
import feedparser
import requests
import json
import pandas as pd
from datetime import datetime, timedelta
import urllib.parse
import random
import time

# ================= 1. åŸºç¤è¨­å®š =================
st.set_page_config(
    page_title="ğŸ‡¹ğŸ‡¼å°ç£ç†±é–€è¨è«–",
    page_icon="ğŸ”¥",
    layout="wide"
)

# ================= 2. å®‰å…¨è®€å–é‡‘é‘° =================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âš ï¸ æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹ç¢ºèª Streamlit Secrets è¨­å®šã€‚")
    st.stop()

genai.configure(api_key=API_KEY)

# ================= 3. å®šç¾©æ–°èä¾†æº (å«å‚™æ´ç¶²å€) =================
def get_rss_urls(category):
    base_search = "https://news.google.com/rss/search"
    base_topic = "https://news.google.com/rss/topics"
    suffix = "hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    # ç”¢ç”Ÿä¸»è¦çš„ã€Œç²¾æº–æœå°‹ã€ç¶²å€
    def make_search_url(query):
        query_with_time = f"{query} when:2d"
        encoded_query = urllib.parse.quote(query_with_time)
        return f"{base_search}?q={encoded_query}&scoring=n&{suffix}"

    # å‚™ç”¨ï¼šGoogle å®˜æ–¹åˆ†é¡ ID (æ¯”è¼ƒä¸æœƒè¢«æ“‹ï¼Œä½†å…§å®¹è¼ƒå»£æ³›)
    # é€™äº› ID æ˜¯ Google News å°ç£ç‰ˆçš„å›ºå®šåˆ†é¡
    topic_ids = {
        "æ”¿æ²»": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0FBUWlHZ0pKVERNU0FBUW", # å°ç£
        "è²¡ç¶“": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx6TVdZU0FBUWlHZ0pKVERNU0FBUW", # è²¡ç¶“
        "ç§‘æŠ€": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRGRqTVhZU0FBUWlHZ0pKVERNU0FBUW", # ç§‘æŠ€
        "å¨›æ¨‚": "CAAqKggKIiRDQkFTRlFvSUwyMHZNREpxYW5RU0FBUWlHZ0pKVERNU0FBUW", # å¨›æ¨‚
        "é‹å‹•": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRFp1ZEdvU0FBUWlHZ0pKVERNU0FBUW", # é«”è‚²
        "åœ‹éš›": "CAAqKggKIiRDQkFTRlFvSUwyMHZNRGx1YlY4U0FBUWlHZ0pKVERNU0FBUW", # ä¸–ç•Œ (æš«ä»£)
        "å¥åº·": "CAAqIaHZBAgESHgQAlIICgYI1p2w8wIw8MuzAzC4rYoD" # å¥åº·
    }

    # 1. å®šç¾©é¦–é¸ç¶²å€ (Search)
    primary_url = ""
    if category == "é¦–é ":
        return [
            f"https://trends.google.com/trends/trendingsearches/daily/rss?geo=TW",
            f"https://news.google.com/rss?{suffix}"
        ]
    elif category == "æ”¿æ²»": primary_url = make_search_url("å°ç£æ”¿æ²» ç«‹æ³•é™¢ è¡Œæ”¿é™¢")
    elif category == "è²¡ç¶“": primary_url = make_search_url("å°ç£è‚¡å¸‚ è²¡ç¶“ å°ç©é›» ç‡Ÿæ”¶")
    elif category == "ç§‘æŠ€": primary_url = make_search_url("å°ç£ç§‘æŠ€ åŠå°é«” AI è¼é”")
    elif category == "å¨›æ¨‚": primary_url = make_search_url("å°ç£å¨›æ¨‚æ–°è ç¶²ç´… è—äºº ç›´æ’­")
    elif category == "é‹å‹•": primary_url = make_search_url("ä¸­è¯è·æ£’ NBA å°ç£é‹å‹•")
    elif category == "åœ‹éš›": primary_url = make_search_url("åœ‹éš›æ–°è ç¾åœ‹ æ—¥æœ¬ ä¸­åœ‹")
    elif category == "å¥åº·": primary_url = make_search_url("å¥åº·é†«ç™‚ é£Ÿå®‰ æµæ„Ÿ è…¸ç—…æ¯’")

    # 2. å®šç¾©å‚™æ´ç¶²å€ (Topic)
    backup_url = ""
    if category in topic_ids:
        backup_url = f"{base_topic}/{topic_ids[category]}?{suffix}"
    else:
        backup_url = f"https://news.google.com/rss?{suffix}" # çœŸçš„ä¸è¡Œå°±å›é¦–é 

    # å›å‚³ä¸€å€‹æ¸…å–®ï¼šå…ˆè©¦ä¸»è¦ï¼Œå¤±æ•—å°±è©¦å‚™ç”¨
    return [primary_url, backup_url]

# ================= 4. æ ¸å¿ƒåŠŸèƒ½ï¼šAI åˆ†æ =================
@st.cache_data(ttl=1800) 
def run_analysis(category):
    debug_logs = []
    
    # A. æ¨¡å‹è¨­å®š
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    generation_config = {"temperature": 1, "response_mime_type": "application/json"}
    
    try:
        model = genai.GenerativeModel('gemini-2.5-flash', safety_settings=safety_settings, generation_config=generation_config)
    except:
        model = genai.GenerativeModel('gemini-pro', safety_settings=safety_settings)

    # B. æŠ“å–è³‡æ–™ (å«å‚™æ´é‚è¼¯)
    target_urls = get_rss_urls(category)
    all_raw_data = []
    
    # è¼ªæ›¿ä½¿ç”¨ä¸åŒçš„ User-Agent é™ä½è¢«æ“‹æ©Ÿç‡
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
    ]
    
    headers = {
        "User-Agent": random.choice(user_agents),
        "Accept": "application/rss+xml, application/xml, text/xml, */*",
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
    }
    cookies = {"CONSENT": "YES+"} 

    success_count = 0
    
    for url in target_urls:
        if success_count > 0 and category != "é¦–é ": break # å¦‚æœä¸æ˜¯é¦–é ï¼Œåªè¦æŠ“åˆ°ä¸€å€‹ä¾†æºå°±å¤ äº†(é¿å…æ··åˆå¤ªé›œ)

        try:
            # éš¨æ©Ÿå»¶é² 0.5~1.5 ç§’ï¼Œæ¨¡æ“¬äººé¡è¡Œç‚º
            time.sleep(random.uniform(0.5, 1.5))
            
            response = requests.get(url, headers=headers, cookies=cookies, timeout=10)
            
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                if len(feed.entries) > 0:
                    debug_logs.append(f"âœ… æˆåŠŸæŠ“å–: {url[:40]}...")
                    success_count += 1
                    
                    limit = 25 if category == "é¦–é " else 15
                    for entry in feed.entries[:limit]:
                        traffic = entry.ht_approx_traffic if hasattr(entry, 'ht_approx_traffic') else "N/A"
                        all_raw_data.append({
                            "title": entry.title,
                            "traffic": traffic,
                            "snippet": entry.summary if hasattr(entry, 'summary') else ""
                        })
                else:
                    debug_logs.append(f"âš ï¸ ä¾†æºç„¡å…§å®¹: {url[:40]}...")
            else:
                debug_logs.append(f"âŒ HTTP {response.status_code}: {url[:40]}...")
                
        except Exception as e:
            debug_logs.append(f"âŒ é€£ç·šéŒ¯èª¤: {str(e)}")
            continue

    if not all_raw_data:
        return [], debug_logs

    # C. AI åˆ†æ
    news_json = json.dumps(all_raw_data, ensure_ascii=False)
    
    if category == "é¦–é ":
        task_desc = "è«‹åˆ—å‡º **15-20 å€‹** å°ç£ç¾åœ¨ **å…¨ç¶²æœ€ç†±é–€ã€æœ€æ–°** çš„è¨è«–è©±é¡Œã€‚"
    else:
        task_desc = f"è«‹å°ˆæ³¨æ–¼ **{category}** é ˜åŸŸï¼Œåˆ—å‡º **10-15 å€‹** è©²é ˜åŸŸ **é€™å…©å¤©å…§** æœ€å—é—œæ³¨çš„è­°é¡Œã€‚"

    # JSON ç¯„ä¾‹ (é˜²å‘†)
    json_example = f"""
    [
      {{
        "id": 1,
        "keyword": "è©±é¡Œé—œéµå­—",
        "category": "åˆ†é¡ (å¦‚: {category})",
        "score": 95,
        "volume_label": "è¨è«–é‡ç´š (å¦‚: 5è¬+ æœå°‹ / ç†±è­°ä¸­)",
        "summary": "ç°¡çŸ­èªªæ˜",
        "hashtags": ["#tag1"]
      }}
    ]
    """

    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ç£ç¤¾ç¾¤è¶¨å‹¢è§€å¯Ÿå®¶ã€‚è«‹åˆ†æä»¥ä¸‹è³‡æ–™ã€‚
    {task_desc}

    åŸå§‹è³‡æ–™ï¼š
    {news_json}
    
    è¦æ±‚ï¼š
    1. **æ™‚æ•ˆå„ªå…ˆ**ï¼šè«‹å¿½ç•¥èˆŠèã€‚
    2. åˆä½µé‡è¤‡çš„äº‹ä»¶ã€‚
    3. æœ‰æµé‡æ•¸æ“šåˆ†æ•¸çµ¦é«˜ã€‚
    4. ç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚
    
    è«‹å›å‚³ JSON Arrayï¼š
    {json_example}
    """
    
    try:
        response = model.generate_content(prompt)
        cleaned_text = response.text.replace("```json", "").replace("```", "").strip()
        if not cleaned_text: return [], debug_logs
        return json.loads(cleaned_text), debug_logs
    except Exception as e:
        return [], [str(e)]

# ================= 5. ä»‹é¢é¡¯ç¤º (UI) =================

st.sidebar.title("ğŸ”¥ å°è¦½é¸å–®")
st.sidebar.markdown("è«‹é¸æ“‡æ‚¨æ„Ÿèˆˆè¶£çš„çœ‹ç‰ˆï¼š")

options = ["é¦–é  (å…¨ç¶²ç†±æœ)", "âš–ï¸ æ”¿æ²»", "ğŸ’° è²¡ç¶“", "ğŸ’» ç§‘æŠ€", "ğŸ¿ å¨›æ¨‚", "âš¾ é‹å‹•", "ğŸŒ åœ‹éš›", "ğŸ¥ å¥åº·"]
selection = st.sidebar.radio("Go to", options, label_visibility="collapsed")

category_map = {
    "é¦–é  (å…¨ç¶²ç†±æœ)": "é¦–é ", "âš–ï¸ æ”¿æ²»": "æ”¿æ²»", "ğŸ’° è²¡ç¶“": "è²¡ç¶“",
    "ğŸ’» ç§‘æŠ€": "ç§‘æŠ€", "ğŸ¿ å¨›æ¨‚": "å¨›æ¨‚", "âš¾ é‹å‹•": "é‹å‹•",
    "ğŸŒ åœ‹éš›": "åœ‹éš›", "ğŸ¥ å¥åº·": "å¥åº·"
}
current_category = category_map[selection]

col1, col2 = st.columns([3, 1])
with col1:
    if current_category == "é¦–é ":
        st.title("ğŸ‡¹ğŸ‡¼ å°ç£ç†±é–€è¨è«–")
    else:
        st.title(f"ğŸ‡¹ğŸ‡¼ {current_category}ç†±é–€è¨è«–")
    st.caption(f"å³æ™‚ AI è¼¿æƒ…åˆ†æ | è³‡æ–™ç¯„åœ: 48å°æ™‚å…§ | æ›´æ–°: {datetime.now().strftime('%H:%M')}")

with col2:
    if st.button("ğŸ”„ é‡æ–°æ•´ç†"):
        st.rerun()

st.divider()

with st.spinner(f'ğŸ” æ­£åœ¨æƒæ {current_category} ç‰ˆé¢æ–°è...'):
    trends, logs = run_analysis(current_category)

if trends:
    st.markdown("""
    <style>
        a.trend-link { text-decoration: none !important; color: inherit !important; display: block; }
        .trend-row { background-color: white; padding: 15px; border-radius: 10px; margin-bottom: 12px; border: 1px solid #eee; transition: all 0.2s ease; cursor: pointer; }
        .trend-row:hover { transform: translateY(-2px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); border-color: #FF4B4B; }
        .rank-num { font-size: 1.5em; font-weight: bold; color: #ccc; width: 40px; text-align: center; }
        .rank-1 { color: #FF4B4B; }
        .rank-2 { color: #FF8F00; }
        .rank-3 { color: #FFC107; }
        .volume-badge { background-color: #ffebee; color: #c62828; padding: 3px 8px; border-radius: 12px; font-size: 0.8em; font-weight: bold; }
        .category-badge { background-color: #f1f3f4; color: #555; padding: 3px 8px; border-radius: 4px; font-size: 0.8em; }
    </style>
    """, unsafe_allow_html=True)

    if current_category == "é¦–é ":
        st.subheader("ğŸ“Š è©±é¡Œç†±åº¦åˆ†ä½ˆ")
        df = pd.DataFrame(trends)
        if not df.empty:
            st.bar_chart(df.set_index('keyword')['score'], color="#FF4B4B")

    st.subheader(f"ğŸ† {current_category}è©±é¡Œæ’è¡Œæ¦œ")
    
    for i, item in enumerate(trends):
        rank_class = f"rank-{i+1}" if i < 3 else ""
        search_query = urllib.parse.quote(item['keyword'])
        google_url = f"https://www.google.com/search?q={search_query}"
        
        st.markdown(f"""
        <a href="{google_url}" target="_blank" class="trend-link">
            <div class="trend-row">
                <div style="display:flex; align-items:center;">
                    <div class="rank-num {rank_class}">{i+1}</div>
                    <div style="flex-grow:1; padding-left:15px;">
                        <div style="display:flex; justify-content:space-between; align-items:center;">
                            <h3 style="margin:0; font-size:1.2em; color:#333;">{item['keyword']} ğŸ”—</h3>
                            <span class="volume-badge">ğŸ”¥ {item.get('volume_label', 'ç†±è­°ä¸­')}</span>
                        </div>
                        <div style="margin-top:5px; font-size:0.9em; color:#666;">
                            <span class="category-badge">{item.get('category', current_category)}</span>
                            <span style="margin-left:8px;">{item['summary']}</span>
                        </div>
                        <div style="margin-top:8px; font-size:0.85em; color:#888;">
                            {' '.join([f'#{tag}' for tag in item.get('hashtags', [])]).replace('##', '#')}
                        </div>
                    </div>
                </div>
            </div>
        </a>
        """, unsafe_allow_html=True)

else:
    st.error("ç›®å‰æµé‡éå¤§ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
    # åªåœ¨çœŸçš„å…¨æ›æ™‚æ‰é¡¯ç¤ºé™¤éŒ¯è³‡è¨Š
    with st.expander("ğŸ› ï¸ ç³»çµ±è¨ºæ–·å ±å‘Š", expanded=True):
        st.write("å˜—è©¦é€£ç·šç´€éŒ„ï¼š")
        for log in logs:
            st.write(log)
