import streamlit as st
import google.generativeai as genai
import feedparser
import requests
import json
import pandas as pd
from datetime import datetime, timedelta
import urllib.parse
import random # ç”¨ä¾†éš¨æ©Ÿå»¶é²

# ================= 1. åŸºç¤è¨­å®š =================
st.set_page_config(
    page_title="ğŸ‡¹ğŸ‡¼å°ç£ç†±é–€è¨è«–",
    page_icon="ğŸ”¥",
    layout="wide"
)

# ================= 2. å®‰å…¨è®€å–é‡‘é‘° =================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âš ï¸ æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹ç¢ºèª Streamlit Secrets è¨­å®šã€‚")
    st.stop()

genai.configure(api_key=API_KEY)

# ================= 3. å®šç¾©æ–°èä¾†æº =================
def get_rss_url(category):
    base_search = "https://news.google.com/rss/search"
    suffix = "hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    def make_search_url(query):
        # åŠ å…¥ when:2d ç¢ºä¿æ–°èæ–°é®®åº¦
        query_with_time = f"{query} when:2d"
        encoded_query = urllib.parse.quote(query_with_time)
        return f"{base_search}?q={encoded_query}&scoring=n&{suffix}"

    topics = {
        "é¦–é ": [
            f"https://trends.google.com/trends/trendingsearches/daily/rss?geo=TW",
            f"https://news.google.com/rss?{suffix}"
        ],
        "æ”¿æ²»": [make_search_url("å°ç£æ”¿æ²» ç«‹æ³•é™¢ è¡Œæ”¿é™¢")],
        "è²¡ç¶“": [make_search_url("å°ç£è‚¡å¸‚ è²¡ç¶“ å°ç©é›» ç‡Ÿæ”¶")],
        "ç§‘æŠ€": [make_search_url("å°ç£ç§‘æŠ€ åŠå°é«” AI è¼é”")],
        "å¨›æ¨‚": [make_search_url("å°ç£å¨›æ¨‚æ–°è ç¶²ç´… è—äºº ç›´æ’­")],
        "é‹å‹•": [make_search_url("ä¸­è¯è·æ£’ NBA å°ç£é‹å‹•")],
        "åœ‹éš›": [make_search_url("åœ‹éš›æ–°è ç¾åœ‹ æ—¥æœ¬ ä¸­åœ‹")],
        "å¥åº·": [make_search_url("å¥åº·é†«ç™‚ é£Ÿå®‰ æµæ„Ÿ è…¸ç—…æ¯’")]
    }
    return topics.get(category, topics["é¦–é "])

# ================= 4. æ ¸å¿ƒåŠŸèƒ½ï¼šAI åˆ†æ =================
# é—œéµä¿®æ”¹ï¼šTTL è¨­å®šç‚º 1800 ç§’ (30åˆ†é˜)
# é€™è¡¨ç¤º 30 åˆ†é˜å…§ï¼Œä¸ç®¡èª°ä¾†è¨ªå•ï¼Œéƒ½ç›´æ¥çµ¦ä»–çœ‹ã€ŒèˆŠçš„å¿«å–ã€ï¼Œä¸è¦å»ç…© Google
@st.cache_data(ttl=1800) 
def run_analysis(category):
    debug_logs = []
    
    # A. æ¨¡å‹è¨­å®š
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    generation_config = {"temperature": 1, "response_mime_type": "application/json"}
    
    try:
        model = genai.GenerativeModel('gemini-2.5-flash', safety_settings=safety_settings, generation_config=generation_config)
    except:
        model = genai.GenerativeModel('gemini-pro', safety_settings=safety_settings)

    # B. æŠ“å–è³‡æ–™
    urls = get_rss_url(category)
    all_raw_data = []
    
    # é—œéµä¿®æ”¹ï¼šæ›´å¼·çš„å½è£ Headers
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://www.google.com/"
    }
    cookies = {"CONSENT": "YES+"} 

    for url in urls:
        try:
            response = requests.get(url, headers=headers, cookies=cookies, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                limit = 30 if category == "é¦–é " else 20
                
                for entry in feed.entries[:limit]:
                    traffic = entry.ht_approx_traffic if hasattr(entry, 'ht_approx_traffic') else "N/A"
                    all_raw_data.append({
                        "title": entry.title,
                        "traffic": traffic,
                        "snippet": entry.summary if hasattr(entry, 'summary') else ""
                    })
            else:
                debug_logs.append(f"HTTP Error: {response.status_code} at {url}")
        except Exception as e:
            debug_logs.append(str(e))
            continue

    if not all_raw_data:
        return [], debug_logs

    # C. AI åˆ†æ
    news_json = json.dumps(all_raw_data, ensure_ascii=False)
    
    if category == "é¦–é ":
        task_desc = "è«‹åˆ—å‡º **15-20 å€‹** å°ç£ç¾åœ¨ **å…¨ç¶²æœ€ç†±é–€ã€æœ€æ–°** çš„è¨è«–è©±é¡Œã€‚"
    else:
        task_desc = f"è«‹å°ˆæ³¨æ–¼ **{category}** é ˜åŸŸï¼Œåˆ—å‡º **10-15 å€‹** è©²é ˜åŸŸ **é€™å…©å¤©å…§** æœ€å—é—œæ³¨çš„è­°é¡Œã€‚"

    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ç£ç¤¾ç¾¤è¶¨å‹¢è§€å¯Ÿå®¶ã€‚è«‹åˆ†æä»¥ä¸‹è³‡æ–™ï¼Œæ‰¾å‡ºã€Œç¾åœ¨é€²è¡Œå¼ã€çš„ç†±é–€è©±é¡Œã€‚
    {task_desc}

    åŸå§‹è³‡æ–™ï¼š
    {news_json}
    
    è¦æ±‚ï¼š
    1
